# -*- coding: utf-8 -*-
import os
import sys
import json
import time
import numpy as np
from typing import List, Tuple, Dict, Optional
from pathlib import Path

if sys.stdout.encoding != 'utf-8':
    sys.stdout.reconfigure(encoding='utf-8')

try:
    from sentence_transformers import SentenceTransformer
    import faiss
except ImportError:
    raise ImportError(
        "–¢—Ä–µ–±—É—é—Ç—Å—è –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏: pip install sentence-transformers faiss-cpu numpy"
    )

try:
    from rank_bm25 import BM25Okapi
except ImportError:
    print("‚ö†Ô∏è  BM25Okapi –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
    BM25Okapi = None


class EmbeddingsService:
    """
    –ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫ —Å –ø–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ–º –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º.
    """

    def __init__(
        self,
        model_name: str = "intfloat/multilingual-e5-base",
        knowledge_base_path: str = os.path.join(os.path.dirname(
            __file__), "..", "data", "knowledge_base.json"),
        cache_dir: Optional[str] = None,
    ):
        if cache_dir:
            os.environ["HF_HOME"] = cache_dir

        self.model_name = model_name
        self.knowledge_base_path = knowledge_base_path

        print(f"üì• –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å: {model_name}...")
        model_start = time.perf_counter()
        self.model = SentenceTransformer(model_name, device="cpu")
        self.embedding_dim = self.model.get_sentence_embedding_dimension()
        model_elapsed = time.perf_counter() - model_start
        print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∑–∞ {model_elapsed:.2f}s. –†–∞–∑–º–µ—Ä –≤–µ–∫—Ç–æ—Ä–∞: {self.embedding_dim}")

        self.locations = []
        self.sections = []
        self.all_documents = []

        self.semantic_index = None
        self.bm25_index = None

        # –ú–∞–ø–ø–∏–Ω–≥ –≤–æ–ø—Ä–æ—Å–æ–≤ –Ω–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        self.category_keywords = {
            'indications': [
                '–ø–ª–æ—Å–∫–æ—Å—Ç–æ–ø–∏–µ', '—à–ø–æ—Ä–∞', '–≤–∞–ª—å–≥—É—Å', '–∞—Ä—Ç—Ä–æ–∑', '–º–æ–∑–æ–ª–∏', '–¥–∏–∞–±–µ—Ç',
                '–∫–æ–ª–µ–Ω–∏', '—Å–∫–æ–ª–∏–æ–∑', '–æ—Å—Ç–µ–æ—Ö–æ–Ω–¥—Ä–æ–∑', '–±–µ—Ä–µ–º–µ–Ω–Ω–æ—Å—Ç—å', '–≤–∞—Ä–∏–∫–æ–∑',
                '—Ñ–∞—Å—Ü–∏–∏—Ç', '–º–µ—Ç–∞—Ç–∞—Ä–∑–∞–ª–≥–∏—è', '–º–æ–ª–æ—Ç–∫–æ–æ–±—Ä–∞–∑–Ω—ã–µ', '–±–æ–ª—å –≤ —Å–ø–∏–Ω–µ',
                '–ø–æ–¥—Ö–æ–¥—è—Ç –ø—Ä–∏', '–ø–æ–º–æ–≥–∞—é—Ç –ø—Ä–∏', '–∑–∞–±–æ–ª–µ–≤–∞–Ω–∏—è', '–¥–µ—Ñ–æ—Ä–º–∞—Ü–∏–∏'],
            'process': [
                '–º–∞—Ç–µ—Ä–∏–∞–ª—ã', '–≠–í–ê', '—Å–ª–µ–ø–æ–∫', '–ø–æ —Å–ª–µ–ø–∫—É', 'Trives', 'Amcube', '—Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ', '3D', '–º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ', '–∏–∑–≥–æ—Ç–æ–≤–ª–µ–Ω–∏–µ'],
            'delivery': [
                '–¥–æ—Å—Ç–∞–≤–∫–∞', '—Å–∞–º–æ–≤—ã–≤–æ–∑', '–∫—É—Ä—å–µ—Ä–æ–º', '–∑–∞–±—Ä–∞—Ç—å', '–ø–æ–ª—É—á–∏—Ç—å', '–ì–∏–∫–∞–ª–æ 1', '–≤—ã–¥–∞—á–∞'],
            'manufacturing_time': ['—Å—Ä–æ–∫–∏', '–≤—Ä–µ–º—è', '–¥–Ω–µ–π', '–∂–¥–∞—Ç—å', '–∫–∞–∫ –¥–æ–ª–≥–æ'],
            'specialists': ['–≤—Ä–∞—á–∏', '–æ—Ä—Ç–æ–ø–µ–¥—ã', '–∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—è', '–ø—Ä–∏–µ–º', '—Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç'],
            'locations': ['–∞–¥—Ä–µ—Å–∞', '—Ñ–∏–ª–∏–∞–ª—ã', '—Å–∞–ª–æ–Ω—ã', '–≥–æ—Ä–æ–¥–∞', '–≥–¥–µ', '–Ω–∞—Ö–æ–¥–∏—Ç—Å—è'],
            'contacts': ['—Ç–µ–ª–µ—Ñ–æ–Ω', '–∫–æ–Ω—Ç–∞–∫—Ç—ã', '—Å–≤—è–∑—å', 'email', '–º–µ—Å—Å–µ–Ω–¥–∂–µ—Ä'],
            'prices': ['—Ü–µ–Ω—ã', '—Å—Ç–æ–∏–º–æ—Å—Ç—å', '—Ä—É–±–ª–∏', '—Å–∫–∏–¥–∫–∞', '–∞–∫—Ü–∏—è'],

            'payment': ['–æ–ø–ª–∞—Ç–∞', '–ø–ª–∞—Ç–µ–∂–∏', '—Å–ø–æ—Å–æ–±—ã', '—Ä–∞—Å—á–µ—Ç', '–∫–∞—Ä—Ç–∞', '–Ω–∞–ª–∏—á–Ω—ã–µ'],
            'advantages': ['–ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞', '—ç—Ñ—Ñ–µ–∫—Ç', '—Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã', '–ø–æ–ª—å–∑—É', '—É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å', '–∫—Ä–æ–≤–æ–æ–±—Ä–∞—â–µ–Ω–∏–µ'],
            'target_audience': ['–∞—É–¥–∏—Ç–æ—Ä–∏—è', '–Ω–∞–∑–Ω–∞—á–µ–Ω–∏–µ', '–¥–ª—è –∫–æ–≥–æ', '–∫–æ–º—É', '–ø–æ–¥—Ö–æ–¥–∏—Ç', '—Å–ø–æ—Ä—Ç—Å–º–µ–Ω—ã', '—Å–ø–æ—Ä—Ç–∏–≤–Ω–æ–π'],
            'specialists': ['–≤—Ä–∞—á–∏', '–æ—Ä—Ç–æ–ø–µ–¥—ã', '–∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—è', '–ø—Ä–∏–µ–º', '—Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç', '–∑–∞–ø–∏—Å–∞—Ç—å—Å—è', '–∑–∞–ø–∏—Å—å'],
            'mobile_cabinet': ['–≤—ã–µ–∑–¥–Ω—ã–µ', '–≤—ã–µ–∑–¥–Ω–æ–π', '–≤—ã–µ–∑–¥', '—Ä–∞—Å–ø–∏—Å–∞–Ω–∏–µ', '–∑–∞–ø–∏—Å—å –Ω–∞ –≤—ã–µ–∑–¥'],
            'contacts': ['—Ç–µ–ª–µ—Ñ–æ–Ω', '–∫–æ–Ω—Ç–∞–∫—Ç—ã', '—Å–≤—è–∑—å', 'email', '–º–µ—Å—Å–µ–Ω–¥–∂–µ—Ä', '–ø–æ–∑–≤–æ–Ω–∏—Ç—å', '–Ω–∞–ø–∏—Å–∞—Ç—å'],
        }

        self._load_knowledge_base()

    def _load_knowledge_base(self) -> None:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ"""
        if not os.path.exists(self.knowledge_base_path):
            raise FileNotFoundError(
                f"Knowledge base –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {self.knowledge_base_path}")

        load_start = time.perf_counter()
        with open(self.knowledge_base_path, 'r', encoding='utf-8') as f:
            kb = json.load(f)
        load_elapsed = time.perf_counter() - load_start

        self.locations = kb.get('locations', [])
        print(f"üìç –ó–∞–≥—Ä—É–∂–µ–Ω–æ –∞–¥—Ä–µ—Å–æ–≤: {len(self.locations)}")

        self.sections = kb.get('sections', {})
        print(f"üìö –ó–∞–≥—Ä—É–∂–µ–Ω–æ —Å–µ–∫—Ü–∏–π: {len(self.sections)}")
        print(f"üßæ –ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π –∑–∞–Ω—è–ª–∞ {load_elapsed:.2f}s")

        self.all_documents = []

        # –î–æ–±–∞–≤–ª—è–µ–º sections
        section_idx = 0
        for key, section in self.sections.items():
            section_text = f"{section['title']}. {section['content']}"

            self.all_documents.append({
                'id': f"section_{section_idx}",
                'type': 'section',
                'title': section['title'],
                'text': section_text,
                'lines': section.get('lines', []),
                'key': key,
            })
            section_idx += 1

        # –î–æ–±–∞–≤–ª—è–µ–º locations
        for i, loc in enumerate(self.locations):
            location_text = f"–°–∞–ª–æ–Ω ORTOS {loc['city']}. –ê–¥—Ä–µ—Å: {loc['address']}. –ß–∞—Å—ã: {loc['working_hours']}. –¢–µ–ª–µ—Ñ–æ–Ω—ã: {', '.join(loc.get('phones', []))}."

            self.all_documents.append({
                'id': f"location_{i}",
                'type': 'location',
                'city': loc['city'],
                'address': loc['address'],
                'text': location_text,
                'full_text': loc['full_text'],
                'phones': loc.get('phones', []),
                'working_hours': loc.get('working_hours', ''),
            })

        print(f"üìÑ –í—Å–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(self.all_documents)}")

    def _build_indices(self) -> None:
        """–°–æ–∑–¥–∞–µ—Ç –∏–Ω–¥–µ–∫—Å—ã"""
        print(f"\nüî® –°–æ–∑–¥–∞–µ–º –∏–Ω–¥–µ–∫—Å—ã...")
        build_start = time.perf_counter()

        texts = [doc['text'] for doc in self.all_documents]

        print("  üìä –°–æ–∑–¥–∞–µ–º semantic –∏–Ω–¥–µ–∫—Å...")
        semantic_start = time.perf_counter()
        embeddings = self.model.encode(
            texts,
            convert_to_tensor=False,
            show_progress_bar=True,
            batch_size=32,
        )
        embeddings = np.array(embeddings, dtype=np.float32)
        print(f"  üìê Embeddings shape: {embeddings.shape}")
        faiss.normalize_L2(embeddings)

        self.semantic_index = faiss.IndexFlatIP(self.embedding_dim)
        self.semantic_index.add(embeddings)
        semantic_elapsed = time.perf_counter() - semantic_start
        print(f"  ‚úÖ Semantic –∏–Ω–¥–µ–∫—Å: {self.semantic_index.ntotal} –≤–µ–∫—Ç–æ—Ä–æ–≤ –∑–∞ {semantic_elapsed:.2f}s, dim={self.embedding_dim}")

        if BM25Okapi:
            print("  üî§ –°–æ–∑–¥–∞–µ–º BM25 –∏–Ω–¥–µ–∫—Å...")
            bm25_start = time.perf_counter()
            tokenized_texts = [text.lower().split() for text in texts]
            self.bm25_index = BM25Okapi(tokenized_texts)
            bm25_elapsed = time.perf_counter() - bm25_start
            print(f"  ‚úÖ BM25 –∏–Ω–¥–µ–∫—Å —Å–æ–∑–¥–∞–Ω –∑–∞ {bm25_elapsed:.2f}s, –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤={len(tokenized_texts)}")

        total_elapsed = time.perf_counter() - build_start
        print(f"‚è±Ô∏è –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {total_elapsed:.2f}s")

    def _get_category_boost(self, query: str, doc_key: str) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç boost –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –≤ –≤–æ–ø—Ä–æ—Å–µ"""
        query_lower = query.lower()

        if doc_key in self.category_keywords:
            keywords = self.category_keywords[doc_key]
            matches = sum(1 for kw in keywords if kw in query_lower)
            if matches > 0:
                return 1.0 + (matches * 0.15)

        return 1.0

    def search(
        self,
        query: str,
        top_k: int = 5,
        min_score: float = 0.30,
    ) -> List[Tuple[Dict, float]]:
        """
        –ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫ —Å –ø–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ–º –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º.
        """
        if not self.semantic_index:
            raise RuntimeError("–ò–Ω–¥–µ–∫—Å—ã –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã!")
        if not query or not query.strip():
            return []

        print(f"üîé –ó–∞–ø—Ä–æ—Å –ø–æ–∏—Å–∫–∞: '{query.strip()}', top_k={top_k}, min_score={min_score}")
        results = {}

        # ===== SEMANTIC SEARCH =====
        query_embedding = self.model.encode([query], convert_to_tensor=True)
        query_embedding = query_embedding.cpu().numpy()
        faiss.normalize_L2(query_embedding)

        distances, indices = self.semantic_index.search(
            query_embedding, min(top_k * 4, len(self.all_documents)))

        for i, idx in enumerate(indices[0]):
            if idx == -1:
                continue
            doc_id = self.all_documents[idx]['id']
            score = float(distances[0][i])
            results[doc_id] = score

        # ===== BM25 BOOST =====
        if self.bm25_index:
            query_tokens = query.lower().split()
            bm25_scores = self.bm25_index.get_scores(query_tokens)
            max_bm25 = bm25_scores.max()

            if max_bm25 > 0:
                bm25_scores = bm25_scores / max_bm25

                for idx, score in enumerate(bm25_scores):
                    if score > 0.05:
                        doc_id = self.all_documents[idx]['id']
                        if doc_id in results:
                            results[doc_id] = results[doc_id] * \
                                0.6 + score * 0.4
                        else:
                            results[doc_id] = score * 0.3

        # ===== –ü–ï–†–ï–†–ê–ù–ñ–ò–†–û–í–ê–ù–ò–ï –ü–û –ö–ê–¢–ï–ì–û–†–ò–Ø–ú =====
        for doc_id, score in list(results.items()):
            doc = next(
                (d for d in self.all_documents if d['id'] == doc_id), None)
            if doc and doc['type'] == 'section':
                category_boost = self._get_category_boost(query, doc['key'])
                if category_boost != 1.0:
                    print(f"  üéØ Boost –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –¥–ª—è {doc['title']} ({doc['key']}): x{category_boost:.2f}")
                results[doc_id] = score * category_boost

        # ===== –°–û–†–¢–ò–†–û–í–ö–ê –ò –§–ò–õ–¨–¢–†–ê–¶–ò–Ø =====
        sorted_results = sorted(
            results.items(), key=lambda x: x[1], reverse=True)

        output = []
        for doc_id, score in sorted_results:
            if len(output) >= top_k:
                break

            if score < min_score:
                break

            doc = next(
                (d for d in self.all_documents if d['id'] == doc_id), None)
            if not doc:
                continue

            output.append((doc, score))

        if output:
            top_score = output[0][1]
            print(f"üìà –ò—Ç–æ–≥ –ø–æ–∏—Å–∫–∞: {len(output)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, top_score={top_score:.4f}")
            for doc, score in output:
                if doc['type'] == 'section':
                    print(f"  ‚Ä¢ –†–∞–∑–¥–µ–ª: {doc['title']} | score={score:.4f} | key={doc.get('key')}")
                else:
                    print(f"  ‚Ä¢ –°–∞–ª–æ–Ω: {doc['city']} | score={score:.4f} | –∞–¥—Ä–µ—Å={doc['address']}")
        else:
            print("üìâ –ò—Ç–æ–≥ –ø–æ–∏—Å–∫–∞: —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–µ—Ç")

        return output

    def build_indices(self) -> None:
        """–ü—É–±–ª–∏—á–Ω—ã–π –º–µ—Ç–æ–¥ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∏–Ω–¥–µ–∫—Å–æ–≤"""
        self._build_indices()

    def save_indices(self, index_dir: str = os.path.join(os.path.dirname(__file__), "..", "data", "embeddings_v2")) -> None:
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∏–Ω–¥–µ–∫—Å—ã –Ω–∞ –¥–∏—Å–∫"""
        Path(index_dir).mkdir(parents=True, exist_ok=True)

        if self.semantic_index:
            faiss.write_index(self.semantic_index,
                              f"{index_dir}/semantic.faiss")
            print(f"‚úÖ Semantic –∏–Ω–¥–µ–∫—Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω")

        metadata = {
            'model_name': self.model_name,
            'embedding_dim': self.embedding_dim,
            'total_documents': len(self.all_documents),
            'has_bm25': self.bm25_index is not None,
        }

        with open(f"{index_dir}/metadata.json", 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)

        print(f"‚úÖ –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã")

    def load_indices(self, index_dir: str = os.path.join(os.path.dirname(__file__), "..", "data", "embeddings_v2")) -> bool:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏–Ω–¥–µ–∫—Å—ã —Å –¥–∏—Å–∫–∞, –µ—Å–ª–∏ –æ–Ω–∏ —Å—É—â–µ—Å—Ç–≤—É—é—Ç"""
        semantic_path = f"{index_dir}/semantic.faiss"
        metadata_path = f"{index_dir}/metadata.json"

        if not os.path.exists(semantic_path) or not os.path.exists(metadata_path):
            return False

        try:
            load_start = time.perf_counter()
            self.semantic_index = faiss.read_index(semantic_path)
            metadata = {}
            try:
                with open(metadata_path, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
            except Exception as meta_error:
                print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ: {meta_error}")
            load_elapsed = time.perf_counter() - load_start
            print(
                f"‚úÖ Semantic –∏–Ω–¥–µ–∫—Å –∑–∞–≥—Ä—É–∂–µ–Ω: {self.semantic_index.ntotal} –≤–µ–∫—Ç–æ—Ä–æ–≤ –∑–∞ {load_elapsed:.2f}s")
            if metadata:
                print(f"‚ÑπÔ∏è –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–Ω–¥–µ–∫—Å–∞: model={metadata.get('model_name')}, dim={metadata.get('embedding_dim')}, docs={metadata.get('total_documents')}")
            return True
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–Ω–¥–µ–∫—Å–æ–≤: {e}")
            return False

    def get_stats(self) -> Dict:
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏–Ω–¥–µ–∫—Å–æ–≤"""
        return {
            'total_documents': len(self.all_documents),
            'total_locations': len(self.locations),
            'total_sections': len(self.sections),
            'embedding_dim': self.embedding_dim,
            'model_name': self.model_name,
            'has_semantic_index': self.semantic_index is not None,
            'has_bm25_index': self.bm25_index is not None,
        }
